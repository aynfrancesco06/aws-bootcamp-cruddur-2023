# Week 4 â€” Postgres and RDS


### 1. Create RDS Postgres Instance
  
  I've used the prescribed code snippet for provisioning an RDS instance via AWS CLI, this is doable also via the AWS RDS GUI. This code snippet is from the AWS documentation here [link](https://docs.aws.amazon.com/cli/latest/reference/rds/)

```
aws rds create-db-instance \
  --db-instance-identifier cruddur-db-instance \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --engine-version  14.6 \
  --master-username cruddurroot \
  --master-user-password <yourpassword> \
  --allocated-storage 20 \
  --availability-zone us-east-1a \
  --backup-retention-period 0 \
  --port 5432 \
  --no-multi-az \
  --db-name cruddur \
  --storage-type gp2 \
  --publicly-accessible \
  --storage-encrypted \
  --enable-performance-insights \
  --performance-insights-retention-period 7 \
  --no-deletion-protection 

```

- This code will be pasted in the terminal on your Gitpod Codespace. Other part of the code snippet like '**master-username**' and '**master-user-password**' will be different.

- engine-version refers to the version of the database will be used. In this case, we will use postgres 14.6 version.
- availability zone should be set to where you usually provision resources. In my case its use-east-1a
- no multi az to save costs
- storage encrypted for security

After inputting this code. The result will be shown in your AWS RDS. 

![image](https://user-images.githubusercontent.com/56792014/226118774-25c6b74c-0628-4e50-8a43-3cbee71436c8.png)


If the RDS instance is not being used. We can temporarily stop the instance to avoid going beyond the Free Tier and incurring costs.
  
![image](https://user-images.githubusercontent.com/56792014/226118934-525a40ca-b48f-4703-a939-b9dbc4299e09.png)

- We will run the docker compose file and verify that postgre is running on docker properly.


### 2. Create bash scripts for easier DB deployment (either prod or local) and deploy GITPOD_IP to RDS instance

- In the backend flask, we've created a bin folder that will contain all the bash scripts that we will be using for our database actions.
- We will also create a schema.sql where we will load/unload the database. This is created in a folder called db inside the backend-flask folder.
- A seed.sql will also be created alongside the schema.sql which will create a test data inside the created database.
- I've also created a URL connection string to save time from inputting the database credentials again and again. Then run this commands as ENV VAR. Prod connection will be derived from the endpoint generated by the RDS instance. Instead of **@localhost:5432/cruddur**, this endpoint will be used **@cruddur-db-instance.cyirdhpwucb7.us-east-1.rds.amazonaws.com:5432/cruddur**.

    - export CONNECTION_URL="postgresql://postgres:password@localhost:5432/cruddur"
    - gp env CONNECTION_URL="postgresql://postgres:password@localhost:5432/cruddur"
    - export PROD_CONNECTION_URL="postgresql://<username>:<prodpassword>@cruddur-db-instance.cyirdhpwucb7.us-east-1.rds.amazonaws.com:5432/cruddur"
    - gp env PROD_CONNECTION_URL="postgresql://<username>:<prodpassword>@cruddur-db-instance.cyirdhpwucb7.us-east-1.rds.amazonaws.com:5432/cruddur"
   

```
postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&...]
```

![image](https://user-images.githubusercontent.com/56792014/226120733-7f464cf2-0aac-481a-bcf4-6769ca2901d9.png)

**seed.sql**
- This bash script will insert data inside the database.
 
```
-- this file was manually created
INSERT INTO public.users (display_name, handle, cognito_user_id)
VALUES
  ('Andrew Brown', 'andrewbrown' ,'MOCK'),
  ('Andrew Bayko', 'bayko' ,'MOCK');

INSERT INTO public.activities (user_uuid, message, expires_at)
VALUES
  (
    (SELECT uuid from public.users WHERE users.handle = 'andrewbrown' LIMIT 1),
    'This was imported as seed data!',
    current_timestamp + interval '10 day'
  )

```  

  
  
**schema.sql**
- It is good practice to obscure as much details that we have on our database, so in this case there's a need to use an extension called **uuid-ossp**. This allows us to mask the sequential numerical identifier on users with random generated characters called UUID.
- The command to import will be used on the backend directory.  It means that we will dump the contents of db/schema.sql to cruddur db inside the psql client.

```
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
DROP TABLE IF EXISTS public.users;
DROP TABLE IF EXISTS public.activities;


CREATE TABLE public.users (
  uuid UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  display_name text NOT NULL,
  handle text NOT NULL,
  email text NOT NULL,
  cognito_user_id text NOT NULL,
  created_at TIMESTAMP default current_timestamp NOT NULL
);

CREATE TABLE public.activities (
  uuid UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  user_uuid UUID NOT NULL,
  message text NOT NULL,
  replies_count integer DEFAULT 0,
  reposts_count integer DEFAULT 0,
  likes_count integer DEFAULT 0,
  reply_to_activity_uuid integer,
  expires_at TIMESTAMP,
  created_at TIMESTAMP default current_timestamp NOT NULL
);
```


**db-connect.sh**
- Running this bash script will enable us to connect to either prod ( AWS RDS ) or local. Add 'prod' at the end when running the script. i.e(./bin/db-connect prod)

```
#! /usr/bin/bash

if [ "$1" = "prod" ]; then
    echo " Running in prod mode"
    URL=$PROD_CONNECTION_URL
else 
    URL=$CONNECTION_URL
fi


psql $URL
```

**db-create.sh**
- Running this bash script will allow us to create a database inside our postgre
```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-create"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

echo 'db-create'

NO_DB_CONNECTION_URL=$(sed 's/\/cruddur//g' <<<"$CONNECTION_URL")
psql $NO_DB_CONNECTION_URL -c "create database cruddur;"
```

**db-drop.sh**
- Running this bash script will drop/delete the database which in this case is named 'cruddur'
```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-drop"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

echo  "db-drop"

NO_DB_CONNECTION_URL=$(sed 's/\/cruddur//g' <<<"$CONNECTION_URL")

psql $NO_DB_CONNECTION_URL -c "drop database cruddur;"
```


**db-schema-load.sh**
```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-schema-load"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

echo "db schema-load"

schema_path="$(realpath .)/db/schema.sql"
echo $schema_path


if [ "$1" = "prod" ]; then
    echo " This is the production environment."
    URL=$PROD_CONNECTION_URL
else 
    URL=$CONNECTION_URL
fi

psql $URL cruddur < $schema_path
```

                                
**db-seed.sh**
```
#! /usr/bin/bash

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-seed"
printf "${CYAN}==== ${LABEL}${NO_COLOR}\n"

echo "db seed"

seed_path="$(realpath .)/db/seed.sql"

echo $seed_path

psql $CONNECTION_URL cruddur < $seed_path
```


**db-sessions**
- This script will allow us to determine if the processes running are done locally or in prod (AWS RDS)  
  
```
#! /usr/bin/bash
# This will check on processes running on pg_stat_activity

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-sessions"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

if [ "$1" = "prod" ]; then
    echo " Running in prod mode"
    URL=$PROD_CONNECTION_URL
else 
    URL=$CONNECTION_URL
fi

NO_DB_URL=$(sed 's/\/cruddur//g' <<<"$URL")
psql $NO_DB_URL -c "select pid as process_id, \
       usename as user,  \
       datname as db, \
       client_addr, \
       application_name as app,\
       state \
from pg_stat_activity;"
```
                                     
**db-setup**
- This is a script that runs all the previous script needed to re-run the database. db-drop script to drop the existing database, then recreate it with the db-create script. Run the schema-load script to load the schema inside the created database in psql. db-seed script is run last that will create test users inside the database.
                                     
```
#! /usr/bin/bash
-e # stop if it fails at any point
CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-create"
printf "${CYAN}==== ${LABEL}${NO_COLOR}\n"

bin_path="$(realpath .)/bin/"

source "$bin_path/db-drop"
source "$bin_path/db-create"
source "$bin_path/db-schema-load"
source "$bin_path/db-seed"
```
                                     
                      
**db-update-sg-rule**                                     
```
#! /usr/bin/bash
-e

CYAN='\033[1;36m'
NO_COLOR='\033[0m'
LABEL="db-update-sg-rule"
printf "${CYAN}== ${LABEL}${NO_COLOR}\n"

aws ec2 modify-security-group-rules \
    --group-id $DB_SG_ID \
    --security-group-rules "SecurityGroupRuleId=$DB_SG_RULE_ID,SecurityGroupRule={IpProtocol=tcp,FromPort=5432,ToPort=5432,CidrIpv4=$GITPOD_IP/32}"
```                          
- The bash script will allow us to update the SG rule with the IP address of our Gitpod Workspace as our Gitpod Workspace IP changes.
- We set a **GITPOD_IP=$(curl ifconfig.me)** as ENV VAR.
- Then create an inbound rule for Postgres in the RDS instance we've created.
  ![image](https://user-images.githubusercontent.com/56792014/226122027-8b59687e-b707-454a-9f6a-3c0ecb30dcda.png)
- We will also set the security group id and security group rule id for the postgres as ENV VAR so it can easily be modified from the terminal in Gitpod. 
- Security group id can be found in the details section of the related Security Group, while the Security Group Rule ID can be found in the inbound rule for postgre connection.
    - export DB_SG_ID="<security group id>"
    - gp env DB_SG_ID="<security group id>"
    - export DB_SG_RULE_ID="<security group rule id>"
    - gp env DB_SG_RULE_ID="<security group rule id>"


After setting this up, end result will be able to create/recreate the local database and insert data from both local and prod postgre.
  
  
### 3. Installed Postgres driver in Backend App

- Postgres driver's purpose is a means to connect to postgres. In this case we will use psycopg as our Flask backend is made with python and our backend will be constantly be communicating with our Postgre.

- We will add these 2 inside the requirements.txt file
    psycopg[binary]
    psycopg[pool]

- Then run this command to install these packages
pip install -r requirements.txt

We will create a file called db.py inside the lib folder.
We will also create a $CONNECTION_URL env vars and put it in the docker-compose.yaml file.
In our services folder. Some files here will be modified to hook up the psycopg code snippets.
  home_activities.py will be modified as of now.

![image](https://user-images.githubusercontent.com/56792014/226123248-bbae75bb-8b45-43a2-b76b-edb9d760da69.png)


                                     
  
  

### 4. Create a Cognito Trigger that inserts user into database
- Link AWS Cognito with AWS Lambda that will insert user into database.
- We need to authorize the cognito_user_id with the actual user in database, so they know what the actual users are.
![image](https://user-images.githubusercontent.com/56792014/226123587-c1e83a8d-4f9b-4f20-8c6c-e0e2d2c7e90e.png)

                                     
- Create a lambda function to house our code in AWS.
![image](https://user-images.githubusercontent.com/56792014/226123810-bff25fbd-a024-4eda-9c54-53d78f9daebc.png)

- Result should be like this.
![image](https://user-images.githubusercontent.com/56792014/226123826-db544eea-38c1-43ab-acbb-dfb21a3563cb.png)

                                     
                                     
- Create a lambda sub-folder in the aws folder. Create a file called **cruddur-post-confirmation.py** and put the code snippet indicated in the instructions to the file. 

**cruddur-post-confirmation.py**
![image](https://user-images.githubusercontent.com/56792014/226123614-57e98452-7de5-45ed-9019-173df515cf28.png)

- Paste the code in the lambda function 
![image](https://user-images.githubusercontent.com/56792014/226123908-c95aa1c1-258c-4219-8615-b75dc469afcd.png)

- Don't also forget to set the ENV VARS in the lambda function. 
  ![image](https://user-images.githubusercontent.com/56792014/226123960-0330071f-72d1-4ffd-b9a3-234286a3d352.png)

- Set the CONNECTION_URL that points to the RDS instance endpoint with postgresql.
![image](https://user-images.githubusercontent.com/56792014/226124020-83b93f37-1a0d-42d1-89f8-b09d26150fee.png)

- We also need to add a layer in our lambda function to enable the psycopg library in our python code.
- Use the specified layer for your python version and corresponding AWS region here [link](https://github.com/jetbridge/psycopg2-lambda-layer)

**Adding a layer in our lambda function**
![image](https://user-images.githubusercontent.com/56792014/226124403-ce83ea2a-2b7e-4f5b-a8ce-0aea023e4241.png)

**Result after adding the layer**
![image](https://user-images.githubusercontent.com/56792014/226124430-f16f7e7f-e8c4-4353-ac5a-059fcc476e39.png)

  
                                     

### 5. Create new activities with a database insert 
